{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# With Automated Machine Learning\n### Energy Demand Forecasting ###\n\nAutomated ML empowers data scientists like me to identify an end-to-end machine learning pipeline for any problem, and I am able to achieve higher accuracy while spending far less of my time. \n\nIt enables a much larger number of experiments to be run."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from IPython.display import Image\nImage(filename='./WithAutoML.png',width=900)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Introduction\nIn this example, we show how AutoML can be used for energy demand forecasting. Make sure you have executed the [configuration](../../../configuration.ipynb) before running this notebook.\n\nIn this notebook we will walk through the following steps:\n1. Creating an experiment in an existing Workspace\n\n\n2. Ingesting energy data and enriching it with weather data from Azure Open Datasets\n\n    2.1 - Inspect data\n    \n    2.2 - Split the data into train and test sets\n\n\n3. Instantiating AutoMLConfig for Automated ML\n\n\n4. Training your model using local compute\n\n\n5. Exploring the results and retrieve the best model\n    \n    5.1 - View the engineered names for featurized data\n    \n    5.2 - Test the best fitted model\n    \n    5.3 - Calculate accuracy metrics\n    \n    5.4 - Model Explainability: what features matter for the forecast?\n    \n       \n6. Registering and deploying your model\n\n    6.1 Define your entry script & dependencies\n    \n    6.2 Define your deployment configuration"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Section 1. Creating an experiment in an existing workspace \nAs part of the setup you have already created a <b>Workspace</b>. For AutoML you would need to create an <b>Experiment</b>. An <b>Experiment</b> is a named object in a <b>Workspace</b>, which is used to run experiments."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import azureml.core\nimport pandas as pd\nimport numpy as np\nimport logging\nimport warnings\n# Squash warning messages for cleaner output in the notebook\nwarnings.showwarning = lambda *args, **kwargs: None\n\n\nfrom azureml.core.workspace import Workspace\nfrom azureml.core.experiment import Experiment\nfrom azureml.train.automl import AutoMLConfig\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### <font color=\"red\">Action Required #1 - Configuration </font> \n\n<font color=\"red\"> Replace <resource_group>, <workspace_region>, and <workspace_name> with the values from paper slip you received. Make sure you remove the angle brackets. The values should be inside the quotes. </font>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#AutoML Workshop subscription\nsubscription_id = \"ba7979f7-d040-49c9-af1a-7414402bf622\"\nresource_group = \"<resource_group>,\"\nworkspace_region = \"<workspace_region>\"\nworkspace_name = \"<workspace_name>\"\n\nprint(subscription_id)\nprint(resource_group)\nprint(workspace_region)\nprint(workspace_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### <font color=\"red\"> Action Required #2 - Authentication\n    \n<font color=\"red\"> Executing next cell will ask you to go to a url to authenticate. Follow the instructions and copy and paste the code given. Some times space at the end after pasting the code. Once the authentication is successful, you will see a message that says Interactive authentication successfully completed. In this step you are logging into the Azure subscription using your corporate credentials </font>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core import Workspace\n\n# Create the workspace using the specified parameters\nws = Workspace.create(name = workspace_name,\n                      subscription_id = subscription_id,\n                      resource_group = resource_group, \n                      location = workspace_region,\n                      create_resource_group = True,\n                      exist_ok = True)\nws.get_details()\n\n# write the details of the workspace to a configuration file to the notebook library\nws.write_config()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### <font color=\"red\"> Action Required #3 - Experiment Name\n    \n<font color=\"red\"> uncomment (by removing # sign at the front) the experiment_name line (2nd line) and replace the <experiment_name> with your alias </font>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# choose a name for the run history container in the workspace\n#experiment_name = ' <experiment_name>'\n\n# project folder\nproject_folder = './sample_projects/automl-energydemandforecasting'\n\nexperiment=Experiment(ws, experiment_name)\n\noutput = {}\noutput['SDK version'] = azureml.core.VERSION\noutput['Subscription ID'] = ws.subscription_id\noutput['Workspace Name'] = ws.name\noutput['Resource Group'] = ws.resource_group\noutput['Location'] = ws.location\noutput['Project Directory'] = project_folder\noutput['Experiment Name'] = experiment.name\npd.set_option('display.max_colwidth', -1)\npd.DataFrame(data = output, index = ['']).T",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Section 2. Ingesting energy data and enriching it with weather data from Azure Open Datasets Data"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data = pd.read_csv(\"nyc_energy.csv\", parse_dates=['timeStamp'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 2.1 Inspect data\nDisplay the first few rows of the data"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt_df = data.loc[(data.timeStamp>'2016-07-01') & (data.timeStamp<='2016-07-07')]\nplt.plot(plt_df['timeStamp'], plt_df['demand'])\nplt.title('New York City power demand over one week in July 2017')\nplt.xticks(rotation=45)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt_df = data.copy().loc[(data['timeStamp']>='2016-01-01') & (data['timeStamp']<'2017-01-01'), ]\nplt.plot(plt_df['timeStamp'], plt_df['demand'], markersize=1)\nplt.title('Hourly demand in 2016')\nplt.ylabel('demand')\nplt.xticks(rotation=45)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 2.2 Split the data into train and test sets\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# let's take note of what columns means what in the data\ntime_column_name = 'timeStamp'\ntarget_column_name = 'demand'\n\nX_train = data[data[time_column_name] < '2017-02-01']\nX_test = data[data[time_column_name] >= '2017-02-01']\ny_train = X_train.pop(target_column_name).values\ny_test = X_test.pop(target_column_name).values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Section 3. Instantiating AutoMLConfig for Automated ML\n\nHere we are using new task type \"forecasting\" for timeseries data training, and other timeseries related settings to instantiate a AutoMLConfig object.\n\nWhat I love the most of Automated ML is that even if it accelerates my work as data scientist, I still have total <b>Control</b>, <b>Transparency</b>, <b>Visibility</b> on what I am doing with my data, the training piece and all the metrics it is using to evaluate different ML approaches.\n\nBelow there is the <b>Configuration file</b> for submitting an Automated Machine Learning experiment in Azure Machine Learning service.\n\nThis configuration object contains and persists the parameters for configuring the experiment run parameters, as well as the training data to be used at run time.\n\n\n|Property|Description|\n|-|-|\n|**task**|forecasting|\n|**primary_metric**|This is the metric that you want to optimize.<br> Forecasting supports the following primary metrics <br><i>spearman_correlation</i><br><i>normalized_root_mean_squared_error</i><br><i>r2_score</i><br><i>normalized_mean_absolute_error</i>\n|**iterations**|Number of iterations. In each iteration, Auto ML trains a specific pipeline on the given data|\n|**iteration_timeout_minutes**|Time limit in minutes for each iteration.|\n|**X**|(sparse) array-like, shape = [n_samples, n_features]|\n|**y**|(sparse) array-like, shape = [n_samples, ], targets values.|\n|**n_cross_validations**|Number of cross validation splits.|\n|**path**|Relative path to the project folder.  AutoML stores configuration files for the experiment under this folder. You can specify a new empty folder. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### <font color=\"red\"> Action Required #4 - AutoML Configuration </font> \n<font color=\"red\"> AutoML comes with many levers that you can use to configure. This is to give you flexibility and control  \nFor example primary_metric is the metric that AutoML use to optimize the machine learning model it is building. AutoML supports several different primary_metrics. To find out more about all the configuration settings you can leisurely read at ( https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-auto-train ) </font>\n    \nNotice that the task is set to forecasting. We are also passing the training dataset and validation dataset (X_train, y_train that we prepared in section 2.2)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "automl_settings = {\n    \"time_column_name\": time_column_name    \n}\n\n\nautoml_config = AutoMLConfig(task = 'forecasting',\n                             debug_log = 'automl_nyc_energy_errors.log',\n                             primary_metric= 'normalized_root_mean_squared_error',\n                             iterations = 10,\n                             iteration_timeout_minutes = 5,\n                             X = X_train,\n                             y = y_train,\n                             n_cross_validations = 3,\n                             path=project_folder,\n                             verbosity = logging.INFO,\n                            **automl_settings)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 4. Training your model \n\n### 4.1 Using local compute\nSubmitting the configuration will start a new run in this experiment. For local runs, the execution is synchronous.\nYou will see the currently running iterations printing to the console."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "local_run = experiment.submit(automl_config, show_output=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "local_run",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\nRunDetails(local_run).show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 4.2 Using AML Compute\nIn the following cells, you will not train the model against the data you just downloaded using the resources provided by Azure Notebooks. Instead, you will deploy an Azure ML Compute cluster that will download the data and use Auto ML to train multiple models, evaluate the performance and allow you to retrieve the best model that was trained. In other words, all of the training will be performed remotely with respect to this notebook.\n\nAs you will see this is almost entirely done thru configuration, with very little code required."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Create the data loading script for remote compute\nThe Azure Machine Learning Compute cluster needs to know how to get the data to train against. You can package this logic in a script that will be executed by the compute when it starts executing the training.\n\nRun the following cells to locally create the get_data.py script that will be deployed to remote compute. You will also use this script when you want train the model locally.\n\nObserve that the get_data method returns the features (X) and the labels (Y) in an object. This structure is expected later when you will configure Auto ML."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\n\n# create project folder\nif not os.path.exists(project_folder):\n    os.makedirs(project_folder)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile $project_folder/get_data.py\n\nimport pandas as pd\nimport numpy as np\n\ndef get_data():\n    time_column_name = 'timeStamp'\n    target_column_name = 'demand'\n    data = pd.read_csv(\"https://anumamahstorage427978d3d.blob.core.windows.net/data/nyc_energy.csv\", parse_dates=['timeStamp'])\n    X = data[data[time_column_name] < '2017-02-01']\n    y = X.pop(target_column_name).values\n\n\n    return { \"X\" : X, \"y\" : y }",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Create AML Compute Cluster\nNow you are ready to create the compute cluster. Run the following cell to create a new compute cluster (or retrieve the existing cluster if it already exists). The code below will create a CPU based cluster where each node in the cluster is of the size STANDARD_D12_V2, and the cluster will have at most 4 such nodes."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "### Create AML CPU based Compute Cluster\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\ncluster_name = \"cpucluster\"\n\ntry:\n    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n    print('Found existing compute target.')\nexcept ComputeTargetException:\n    print('Creating a new compute target...')\n    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2S_V3',\n                                                           max_nodes=4)\n\n    # create the cluster\n    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n\n    compute_target.wait_for_completion(show_output=True)\n\n# Use the 'status' property to get a detailed status for the current AmlCompute. \n#print(compute_target.status.serialize())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Run the following cell to create the configuration."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "automl_config = AutoMLConfig(task = 'forecasting',\n                             iterations = 10,\n                             iteration_timeout_minutes = 5, \n                             time_column_name =  'timeStamp', \n                             primary_metric = 'normalized_root_mean_squared_error',\n                             n_cross_validations = 3,\n                             debug_log = 'automl.log',\n                             verbosity = logging.DEBUG,\n                             data_script = project_folder + \"/get_data.py\",\n                             compute_target = compute_target,\n                             path = project_folder)\nremote_run = experiment.submit(automl_config, show_output=False)\nremote_run",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Once the above cell completes, the run is starting but will likely have a status of Preparing for you. To wait for the run to complete before continuing (and to view the training status updates as they happen), run the following cell. The first time you run this, it will take about 12-15 minutes to complete as the cluster is configured and then the AutoML job is run. Output will be streamed as the tasks progress):"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "remote_run.wait_for_completion(show_output=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 5. Exploring the results and retrieve the best model\nBelow we select the best pipeline from our iterations. \n\nThe get_output file allows you to retrieve the best run and fitted model for any logged metric or a particular iteration."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_run, fitted_model = local_run.get_output()\nfitted_model.steps",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_run, fitted_model = remote_run.get_output()\nprint(best_run)\nprint(fitted_model)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Widget for monitoring runs\nInitial run could take a few minutes. Depending on the data and number of iterations this can run for while. You will see the currently running iterations printing in the widget. The widget will sit on \"loading\" until the first iteration completed, then you will see an auto-updating graph and table show up. It refreshed once per minute, so you should see the graph update as child runs complete.\n\nNOTE: The widget displays a link at the bottom. Ignore the link for a moment. This links to a web-ui to explore the individual run details."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\nRunDetails(local_run).show() ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\nRunDetails(remote_run).show() ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 5.1 View the engineered names for featurized data\nBelow we display the engineered feature names generated for the featurized data using the time-series featurization."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "fitted_model.named_steps['timeseriestransformer'].get_engineered_feature_names()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 5.2 Test the Best Fitted Model\n\nFor forecasting, we will use the `forecast` function instead of the `predict` function. There are two reasons for this.\n\nWe need to pass the recent values of the target variable `y`, whereas the scikit-compatible `predict` function only takes the non-target variables `X`. In our case, the test data immediately follows the training data, and we fill the `y` variable with `NaN`. The `NaN` serves as a question mark for the forecaster to fill with the actuals. Using the forecast function will produce forecasts using the shortest possible forecast horizon. The last time at which a definite (non-NaN) value is seen is the _forecast origin_ - the last time when the value of the target is known. \n\nUsing the `predict` method would result in getting predictions for EVERY horizon the forecaster can predict at. This is useful when training and evaluating the performance of the forecaster at various horizons, but the level of detail is excessive for normal use."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Replace ALL values in y_pred by NaN. \n# The forecast origin will be at the beginning of the first forecast period\n# (which is the same time as the end of the last training period).\ny_query = y_test.copy().astype(np.float)\ny_query.fill(np.nan)\n# The featurized data, aligned to y, will also be returned.\n# This contains the assumptions that were made in the forecast\n# and helps align the forecast to the original data\ny_fcst, X_trans = fitted_model.forecast(X_test, y_query)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# limit the evaluation to data where y_test has actuals\ndef align_outputs(y_predicted, X_trans, X_test, y_test, predicted_column_name = 'predicted'):\n    \"\"\"\n    Demonstrates how to get the output aligned to the inputs\n    using pandas indexes. Helps understand what happened if\n    the output's shape differs from the input shape, or if\n    the data got re-sorted by time and grain during forecasting.\n    \n    Typical causes of misalignment are:\n    * we predicted some periods that were missing in actuals -> drop from eval\n    * model was asked to predict past max_horizon -> increase max horizon\n    * data at start of X_test was needed for lags -> provide previous periods\n    \"\"\"\n    df_fcst = pd.DataFrame({predicted_column_name : y_predicted})\n    # y and X outputs are aligned by forecast() function contract\n    df_fcst.index = X_trans.index\n    \n    # align original X_test to y_test    \n    X_test_full = X_test.copy()\n    X_test_full[target_column_name] = y_test\n\n    # X_test_full's does not include origin, so reset for merge\n    df_fcst.reset_index(inplace=True)\n    X_test_full = X_test_full.reset_index().drop(columns='index')\n    together = df_fcst.merge(X_test_full, how='right')\n    \n    # drop rows where prediction or actuals are nan \n    # happens because of missing actuals \n    # or at edges of time due to lags/rolling windows\n    clean = together[together[[target_column_name, predicted_column_name]].notnull().all(axis=1)]\n    return(clean)\n\ndf_all = align_outputs(y_fcst, X_trans, X_test, y_test)\ndf_all.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Looking at `X_trans` is also useful to see what featurization happened to the data."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Calculate accuracy metrics\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def MAPE(actual, pred):\n    \"\"\"\n    Calculate mean absolute percentage error.\n    Remove NA and values where actual is close to zero\n    \"\"\"\n    not_na = ~(np.isnan(actual) | np.isnan(pred))\n    not_zero = ~np.isclose(actual, 0.0)\n    actual_safe = actual[not_na & not_zero]\n    pred_safe = pred[not_na & not_zero]\n    APE = 100*np.abs((actual_safe - pred_safe)/actual_safe)\n    return np.mean(APE)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Simple forecasting model\")\nrmse = np.sqrt(mean_squared_error(df_all[target_column_name], df_all['predicted']))\nprint(\"[Test Data] \\nRoot Mean squared error: %.2f\" % rmse)\nmae = mean_absolute_error(df_all[target_column_name], df_all['predicted'])\nprint('mean_absolute_error score: %.2f' % mae)\nprint('MAPE: %.2f' % MAPE(df_all[target_column_name], df_all['predicted']))\n\n# Plot outputs\n%matplotlib notebook\ntest_pred = plt.scatter(df_all[target_column_name], df_all['predicted'], color='b')\ntest_test = plt.scatter(y_test, y_test, color='g')\nplt.legend((test_pred, test_test), ('prediction', 'truth'), loc='upper left', fontsize=8)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We did not use lags in the previous model specification. Automated ML allows you to use `target_lags` and `target_rolling_window_size`.\n\nNow that we configured target lags, that is the previous values of the target variables, and the prediction is no longer horizon-less. We therefore must specify the `max_horizon` that the model will learn to forecast. The `target_lags` keyword specifies how far back we will construct the lags of the target variable, and the `target_rolling_window_size` specifies the size of the rolling window over which we will generate the `max`, `min` and `sum` features."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "automl_settings_lags = {\n    'time_column_name': time_column_name,\n    'target_lags': 1,\n    'target_rolling_window_size': 5,\n    # you MUST set the max_horizon when using lags and rolling windows\n    # it is optional when looking-back features are not used \n    'max_horizon': len(y_test), # only one grain\n}\n\n\nautoml_config_lags = AutoMLConfig(task = 'forecasting',\n                             debug_log = 'automl_nyc_energy_errors.log',\n                             primary_metric='normalized_root_mean_squared_error',\n                             iterations = 10,\n                             iteration_timeout_minutes = 5,\n                             X = X_train,\n                             y = y_train,\n                             n_cross_validations = 3,\n                             path=project_folder,\n                             verbosity = logging.INFO,\n                            **automl_settings_lags)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "local_run_lags = experiment.submit(automl_config_lags, show_output=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_run_lags, fitted_model_lags = local_run_lags.get_output()\ny_fcst_lags, X_trans_lags = fitted_model_lags.forecast(X_test, y_query)\ndf_lags = align_outputs(y_fcst_lags, X_trans_lags, X_test, y_test)\ndf_lags.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Forecasting model with lags\")\nrmse = np.sqrt(mean_squared_error(df_lags[target_column_name], df_lags['predicted']))\nprint(\"[Test Data] \\nRoot Mean squared error: %.2f\" % rmse)\nmae = mean_absolute_error(df_lags[target_column_name], df_lags['predicted'])\nprint('mean_absolute_error score: %.2f' % mae)\nprint('MAPE: %.2f' % MAPE(df_lags[target_column_name], df_lags['predicted']))\n\n# Plot outputs\n%matplotlib notebook\ntest_pred = plt.scatter(df_lags[target_column_name], df_lags['predicted'], color='b')\ntest_test = plt.scatter(y_test, y_test, color='g')\nplt.legend((test_pred, test_test), ('prediction', 'truth'), loc='upper left', fontsize=8)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 5.4 Model Explainability: What features matter for the forecast?\n\nI can also use model explainability with Automated ML - This gives me transparency into how the model was built and what features has the most influence on the prediction.  \n \nThe informative features make all sorts of intuitive sense. Temperature is a strong driver of heating and cooling demand in NYC. Apart from that, the daily life cycle, expressed by `hour`, and the weekly cycle, expressed by `wday` drives people's energy use habits. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.automl.automlexplainer import explain_model\n\n# feature names are everything in the transformed data except the target\nfeatures = X_trans.columns[:-1]\nexpl = explain_model(fitted_model, X_train, X_test, features = features, best_run=best_run_lags, y_train = y_train)\n# unpack the tuple\nshap_values, expected_values, feat_overall_imp, feat_names, per_class_summary, per_class_imp = expl\nbest_run_lags",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Please go to the Azure Portal's best run to see the top features chart.\n\nThe informative features make all sorts of intuitive sense. Temperature is a strong driver of heating and cooling demand in NYC. Apart from that, the daily life cycle, expressed by `hour`, and the weekly cycle, expressed by `wday` drives people's energy use habits."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 6. Registering and deploying your model\nThe model registry is a way to store and organize your trained models in the Azure cloud. Models are registered in your Azure Machine Learning service workspace."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "description = 'AutoML NYC Energy forecaster'\ntags = None\nmodel = local_run_lags.register_model(description = description, tags = tags)\n\nprint(local_run_lags.model_id)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 6.1 Define your entry script & dependencies\nThe script contains two functions that load and run the model:\n\n- *init()*: Typically this function loads the model into a global object. This function is run only once when the Docker container for your web service is started.\n- *run(input_data)*: This function uses the model to predict a value based on the input data. Inputs and outputs to the run typically use JSON for serialization and de-serialization. You can also work with raw binary data. You can transform the data before sending to the model, or before returning to the client.\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### <font color = \"red\"> Action Required #5 - Model ID </font>\n<font color = \"red\"> Replace the '<< modelid >>' name on line 13 with the id generated in the previous step.  </font>\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile score_fcast.py\nimport pickle\nimport json\nimport numpy as np\nimport pandas as pd\nimport azureml.train.automl\nfrom sklearn.externals import joblib\nfrom azureml.core.model import Model\n\n\ndef init():\n    global model\n    model_path = Model.get_model_path(model_name = '<< modelid >>') # this name is model.id of model that we want to deploy\n    # deserialize the model file back into a sklearn model\n    model = joblib.load(model_path)\n\ntimestamp_columns = ['timeStamp']\n\ndef run(rawdata, test_model = None):\n    \"\"\"\n    Intended to process 'rawdata' string produced by\n    \n    {'X': X_test.to_json(), y' : y_test.to_json()}\n    \n    Don't convert the X payload to numpy.array, use it as pandas.DataFrame\n    \"\"\"\n    columns = []\n    target = \"\"\n    try:\n        # unpack the data frame with timestamp     \n        rawobj = json.loads(rawdata)                    # rawobj is now a dict of strings\n        y_pred = np.array(rawobj['y'])                  # reconstitute numpy array from serialized list\n        # We do not have grain columns so we may needd to add them.\n        grain_column_names = model.grain_column_names if test_model is None else test_model.grain_column_names\n        X_pred = pd.read_json(rawobj['X'], convert_dates=False)   # load the pandas DF from a json string\n        if grain_column_names[0] not in X_pred.columns:\n            X_pred[grain_column_names[0]]=grain_column_names[0]\n        X_pred.sort_index(inplace=True)\n        for col in timestamp_columns:                             # fix timestamps\n            if X_pred.dtypes[col] == \"object\":\n                # Handle the string conversion to date.\n                X_pred[col] = pd.to_datetime(X_pred[col]) \n            else:\n                X_pred[col] = pd.to_datetime(X_pred[col], unit='ms') \n        X_copy = X_pred.copy()           \n        \n        if test_model is None:\n            result = model.forecast(X_pred, y_pred)       # use the global model from init function\n            target = model._ts_transformer.target_column_name\n            index = [model.time_column_name] + model.grain_column_names\n        else:\n            result = test_model.forecast(X_pred, y_pred)  # use the model on which we are testing\n            target = test_model._ts_transformer.target_column_name\n            index = [test_model.time_column_name] + test_model.grain_column_names\n        \n    except Exception as e:\n        result = str(e)\n        return json.dumps({\"error\": result})\n    \n    # Make sure forecast is sorted in the same orger as an input, so the output Y will align.\n    dfForecast = result[1]\n    dfForecast.reset_index(drop=False, inplace=True)\n    dfForecast = dfForecast[index + [target]]\n    # Make sure index is the same.\n    X_copy['ix'] = X_copy.index\n    dfForecast = X_copy.merge(dfForecast, how='left', on=index)\n    dfForecast.set_index('ix', drop=True, inplace=True)\n    dfForecast.sort_index(inplace=True)\n    dfForecast = dfForecast[timestamp_columns + [target]]\n    target_vals = dfForecast.pop(target).values\n    forecast_as_list = target_vals.tolist()\n    return json.dumps({\"forecast\": forecast_as_list,   # return the minimum over the wire: \n                       \"index\": dfForecast.to_json()  # no forecast and its featurized values\n                      })",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# get the model\nfrom azureml.train.automl.run import AutoMLRun\n\nexperiment = Experiment(ws, experiment_name)\nml_run = AutoMLRun(experiment = experiment, run_id = local_run_lags.id)\nbest_iteration = int(str.split(best_run_lags.id,'_')[-1])      # the iteration number is a postfix of the run ID.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# get the best model's dependencies and write them into this file\nfrom azureml.core.conda_dependencies import CondaDependencies\n\nconda_env_file_name = 'fcast_env.yml'\n\ndependencies = ml_run.get_run_sdk_dependencies(iteration = best_iteration)\nfor p in ['azureml-train-automl', 'azureml-sdk', 'azureml-core']:\n    print('{}\\t{}'.format(p, dependencies[p]))\n\nmyenv = CondaDependencies.create(conda_packages=['numpy','scikit-learn'], pip_packages=['azureml-sdk[automl]'])\n\nmyenv.save_to_file('.', conda_env_file_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### <font color = \"red\"> Action Required #7 - Model ID</font>\n<font color = \"red\"> Replace the '<< modelid >>' name on last with the id generated.  </font>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# this is the script file name we wrote a few cells above\nscript_file_name = 'score_fcast.py'\n\n# Substitute the actual version number in the environment file.\n# This is not strictly needed in this notebook because the model should have been generated using the current SDK version.\n# However, we include this in case this code is used on an experiment from a previous SDK version.\n\nwith open(conda_env_file_name, 'r') as cefr:\n    content = cefr.read()\n\nwith open(conda_env_file_name, 'w') as cefw:\n    cefw.write(content.replace(azureml.core.VERSION, dependencies['azureml-sdk']))\n\n# Substitute the actual model id in the script file.\n\nwith open(script_file_name, 'r') as cefr:\n    content = cefr.read()\n\nwith open(script_file_name, 'w') as cefw:\n    cefw.write(content.replace('<< modelid >>' , local_run_lags.model_id))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.image import Image, ContainerImage\n\nimage_config = ContainerImage.image_configuration(runtime= \"python\",\n                                 execution_script = script_file_name,\n                                 conda_file = conda_env_file_name,\n                                 tags = {'type': \"automl-energy-forecasting\"},\n                                 description = \"Image for ny energy forecasting sample\")\n\nimage = Image.create(name = \"automl-energy-fcast-image\",\n                     # this is the model object \n                     models = [model],\n                     image_config = image_config, \n                     workspace = ws)\n\nimage.wait_for_creation(show_output = True)\n\nif image.creation_state == 'Failed':\n    print(\"Image build log at: \" + image.image_build_log_uri)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 6.2 Define your deployment configuration\nBefore deploying, you must define the deployment configuration. The deployment configuration is specific to the compute target that will host the web service. For example, when deploying locally you must specify the port where the service accepts requests.\n\nHere we use Azure Container Instances to run serverless Docker containers in Azure with simplicity and speed. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.webservice import AciWebservice\n\naciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n                                               memory_gb = 2, \n                                               tags = {'type': \"automl-energy-forecasting\"},\n                                               description = \"Automl ny energy forecasting service\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### <font color = \"red\"> Action Required #8 - ACI Service Name</font>\n<font color = \"red\"> Uncomment the 'aci_service_name' and add your alias there. </font>"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.webservice import Webservice\n\n#aci_service_name = 'automl-energy-fc-01'\nprint(aci_service_name)\n\naci_service = Webservice.deploy_from_image(deployment_config = aciconfig,\n                                           image = image,\n                                           name = aci_service_name,\n                                           workspace = ws)\naci_service.wait_for_deployment(True)\nprint(aci_service.state)\naci_service.scoring_uri",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(aci_service.get_logs())",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "xiaga, tosingli"
      }
    ],
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}